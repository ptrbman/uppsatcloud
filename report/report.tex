\documentclass{article}

\input{preamble}
\usepackage{lipsum}
\AtBeginDocument{\renewcommand{\bibname}{\centerline References}}

% Don't hyphenate UppSAT
\hyphenation{UppSAT}

\begin{document}

\input{title}

\begin{multicols}{2}

\section*{Abstract}

\textbf{ \textit { \fixme{This paper describes a system for efficiently
      automating and scheduling a series of compute-heavy benchmarks, as
      exemplified by the SMT~solver {UppSAT}, using Docker containers on a
      virtualised Cloud platform based on OpenStack. It finds that switching to
      virtualised benchmarking immediately solves the $P=NP$ problem, cures
      cancer, and brings about world peace, in addition to providing an
      aesthetically perfect solution.} }}

\section*{Background}

SMT~solvers are used in different areas, with one important use case being
verification of hardware and software. For example, in model checking,
SMT~formulas are generated such that any solution corresponds to a hardware or
software bug.

Many research topics in SMT~solvers correspond to improving solver techniques
and devising new strategies and improved parameters of search. However, as the
problem is NP-hard, such improvements are typically a trade off with some cases
becoming faster and others slower. Therefore, when evaluating new methods
extensive benchmarking must be used.

These benchmarks are often run on local clusters or even local machines in
sequential order, leading to long waits from start until a complete picture of
the effects of the evaluated modifications are yielded. By instead using a cloud
approach, where not only the number of machines could be scaled, but also
dynamic load-balancing could be used, this waiting time could be greatly
reduced.

We therefore investigate how a SMT~solver can be adapted for benchmarking in a
cloud environment, with UppSAT~\cite{uppsat} as a case study. The produced
solution represents a partially automated testbed environment in the cloud with
a complete continuous integration pipeline from code pushed to Git to it being
benchmarked in the cloud, as well as a Representational State Transfer (REST)
API controlling the testbed environment. Moreover, a modular system of Docker
containers~\cite{docker} is used to package the solver for the test cases.

We argue that the use of Docker serves the dual purpose of enhancing both
repeatability and traceability of the experiments, as the entire experiment
environment is packaged into containers, allowing the use of Docker's tools for
comparison and analysis of containers in the event of unexpected results.
Additionally, as an added benefit, Docker ensures clean separation of
experiments.

Finally, we investigate how virtualisation affects measurement errors of
benchmarks, a topic in which there is a dearth of previous research.

\section*{Methods}

Our system, Testbench, is an interactive batch processing system, where
experiments are submitted and then executed. An experiment consists of a set of
approximation levels for UppSAT, a list of Docker containers with solvers to
benchmark, a timeout (applied to all configurations), and a set of benchmarks to
execute. The system then evaluates the cross product of all these configuration
options (corresponding to a set of trials) across all solvers and reports on the
results.

\subsection*{REST API Design}

This functionality is represented as two REST endpoints \texttt{/experiment},
and \texttt{/experiment/<ID>}. A \texttt{POST}~request (illustrating
non-idempotency) to \texttt{/experiment} will start a new experiment, assigning
it a unique ID, and redirecting the user to \texttt{/experiment/<ID>}, where it
can be queried for its status (using a \texttt{GET}~request). Experiments can
then be canceled (if running) and deleted via the \texttt{DELETE}~request to
their endpoint.

\subsection*{System Design and Implementation}

The implementation is based on the Celery task queue~\cite{celery}, and packaged
in Docker~\cite{docker} containers. The front-end REST API receives input from
the user, unpacks the experiment into a set of trials, and submits them to the
Celery task queue (via RabbitMQ) for processing. Each worker, with identical
configuration, listens on the task queue, fetches tasks, and executes them,
storing the results back into Celery (via Redis). The user can then, via the
front-end REST API, query experiments for progress as they are executing, and is
also able to cancel tasks via the \texttt{DELETE} command. An illustration of
the architecture can be seen in Figure~\ref{fig:architecture}.

\begin{Figure}
  \centering
  \includegraphics[width=0.9\textwidth]{architecture}
  \captionof{figure}{An overview of the architecture of Testbench.}\label{fig:architecture}
\end{Figure}

As experiments are executed inside of Docker containers, the host machine's
control socket is bind-mounted into the Docker container running the workers,
such that the worker effectively spawns and runs experiments at the same
containment level as itself. The runtime reported is the runtime seen by Docker.

\subsection*{Cloud Environment and Provisioning}

Testbench is configured in a OpenStack cloud environment~\cite{openstack}, using
HEAT orchestration templates for provisioning. The base VMs are running
Ubuntu~\fixme{18.08}. RabbitMQ, Redis, and the front-end API are all installed
on a (\texttt{small}) separate VM (called the master) in order to not interfere
with the experiments. Each worker VM (\texttt{medium}) receives the same
configuration; a clean installation of the Docker community edition, the address
of the master VM, and a Docker container mounting a shared folder via NFS from
the master for sharing benchmark files. All workers immediately start up a
container running a worker instance upon booting and finishing provisioning.
These worker instances were intentionally configured to only execute one task at
a time, for optimal isolation between trials.

It is worth mentioning that Testbench is designed to be relatively independent
of the underlying architecture. Therefore, it can easily be executed on ``bare
metal'' hardware as well as on virtual machines.

\section*{Results}

\subsection*{Measurement Errors in Our Cloud Environment}
\fixme{In which we report on the standard deviation and therefore the
  reliability of our experiments when run on virtualised hardware and ideally
  find that it's as close to zero as possible.}


\subsection*{Benchmark Lead Time Comparison}
 \fixme{Here we do some comparison on how long it takes to finish a set of benchmarks,
   compared to going by the manual approach, ideally finding that it's as close
   to positive infinity as possible.}


\section*{Discussion}

\fixme{\lipsum[66]}

\section*{Conclusion}

\fixme{\lipsum[100]}

\printbibliography

\end{multicols}
\end{document}
